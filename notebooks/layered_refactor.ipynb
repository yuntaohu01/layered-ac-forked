{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71d71d6e-c88d-474e-bdd3-f3777d41278e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a83936e-c8fe-419f-818b-3e167b164ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import cvxpy as cp\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f329dac-5098-485f-8bd3-47b5a9e0d271",
   "metadata": {},
   "source": [
    "## Linear Unconstrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49705331-3124-4094-9e13-3d5bb5f265cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear env\n",
    "from env.dynamics import linear\n",
    "from env.costs import quadratic\n",
    "from controller.ilqr import ILQRHelper\n",
    "from env.gymenv import GymEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd6d8bb5-2d6c-44e5-b019-115cee6a8d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "seed = 1\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "_ = torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bfb19eaa-12eb-48b8-8bf3-7b7148e83921",
   "metadata": {},
   "outputs": [],
   "source": [
    "Nx = 4\n",
    "Nu = 4\n",
    "T = 20\n",
    "RQratio = 1e-2\n",
    "u_max = 2\n",
    "u_min = 2\n",
    "rho = 2\n",
    "\n",
    "# Define Dynamics\n",
    "dynamics = linear.sample_linear_dynamics(Nx, Nu, A_norm=1.0)\n",
    "\n",
    "# Define Cost\n",
    "cost = quadratic.QuadraticCost(\n",
    "    torch.eye(dynamics.Nx),\n",
    "    RQratio * torch.eye(dynamics.Nu)\n",
    ")\n",
    "\n",
    "# Define how to sample x0\n",
    "reset_fn = lambda: torch.randn(dynamics.Nx)\n",
    "\n",
    "u_max = 10\n",
    "u_min = -10\n",
    "u_range = u_max - u_min\n",
    "nominal_env = GymEnv(dynamics, cost, reset_fn, T, ulim=[u_min, u_max])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "790a9640-c29b-416a-8173-74bb5000939f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Specify the architecture of networks\n",
    "from agent.policy import LinearActor\n",
    "from agent.qnetwork import QuadraticQNetwork\n",
    "\n",
    "actor_creation_fn = lambda obs_dim, act_dim: LinearActor(obs_dim, act_dim)\n",
    "qnetwork_creation_fn = lambda obs_dim, act_dim: QuadraticQNetwork(obs_dim, act_dim)\n",
    "\n",
    "def dual_network_creation_fn(nominal_obs_dim, obs_dim):\n",
    "    dual_network = nn.Linear(\n",
    "        nominal_obs_dim, obs_dim - nominal_obs_dim,\n",
    "        bias=False\n",
    "    )\n",
    "    dual_network.weight.data.zero_()\n",
    "    return dual_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c656f5b7-3cfb-4207-bbf2-e0b3cae167aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Specify how to sample new trajectories\n",
    "def ref_traj_gen_fn(x0s, T, agent, global_step):\n",
    "    b, Nx = x0s.shape\n",
    "    assert b == 1, \"Can only handle one initial condition at a time\"\n",
    "    x0 = x0s[0]\n",
    "    Q = agent.nominal_env.cost_fn.Q\n",
    "    K = agent.actor.gain.weight.data.cpu().numpy()\n",
    "    P = agent.qf1.get_P().cpu().numpy()\n",
    "    P = (P + P.T) / 2\n",
    "\n",
    "    r = cp.Variable(Nx * (T + 1))\n",
    "    v = agent.dual_network(torch.tensor(x0, device=agent.device)).detach().cpu().numpy()\n",
    "    if agent.args.no_dual:\n",
    "        v = np.zeros_like(v)\n",
    "    rtilde = r + v\n",
    "    xr = cp.hstack([x0, rtilde])\n",
    "    u = K @ xr\n",
    "    xru = cp.hstack([xr, u])\n",
    "    objective = cp.quad_form(r, np.kron(np.eye(T+1), Q.numpy()))\n",
    "    objective += cp.quad_form(xru, cp.psd_wrap(P))\n",
    "    prob = cp.Problem(cp.Minimize(objective), [])\n",
    "    try:\n",
    "        prob.solve()\n",
    "    except Exception as e:\n",
    "        print(f\"Error occured when solving the QP: {e}\")\n",
    "        print(np.linalg.svd(P))\n",
    "    return torch.tensor(rtilde.value).float().unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f7556bbe-0cb8-4750-a5a9-81bf2efbb079",
   "metadata": {},
   "outputs": [],
   "source": [
    "## How to find optimal cost\n",
    "A = nominal_env.dynamics.A.numpy()\n",
    "B = nominal_env.dynamics.B.numpy()\n",
    "Q = nominal_env.cost_fn.Q.numpy()\n",
    "R = nominal_env.cost_fn.R.numpy()\n",
    "nominal_Nx, Nu = B.shape\n",
    "T = nominal_env.T\n",
    "\n",
    "def riccati(A, B, Q, R, T):\n",
    "    P = [None] * (T + 1)\n",
    "    K = [None] * (T)\n",
    "    P[T] = Q  # Final cost\n",
    "    for t in range(T - 1, -1, -1):\n",
    "        K[t] = -np.linalg.inv(R + B.T @ P[t + 1] @ B) @ B.T @ P[t + 1] @ A\n",
    "        P[t] = Q + A.T @ P[t + 1] @ A + A.T @ P[t + 1] @ B @ K[t]\n",
    "    return P, K\n",
    "    \n",
    "nominal_P, nominal_K = riccati(A, B, Q, R, T)\n",
    "opt_cost_fn = lambda x0: x0 @ (nominal_P[0] @ x0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "113fcf64-800e-4e45-a837-3ef5e8da61e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extra validation: optimal map theta\n",
    "def optimal_dual_map(A, B, Q, R, T, rho):\n",
    "    # Compute Theta^* of the system (optimal dual map)\n",
    "    Nx, Nu = B.shape\n",
    "    E = np.zeros(((T + 1) * Nx, T * Nu))\n",
    "    for i in range(T+1):\n",
    "        for j in range(0, i):\n",
    "            E[i*Nx:(i+1)*Nx, j*Nu:(j+1)*Nu] = np.linalg.matrix_power(A, i-j-1) @ B\n",
    "    F = np.vstack([np.linalg.matrix_power(A, i) for i in range(T+1)])\n",
    "    QQ = np.kron(np.eye(T+1), Q)\n",
    "    RR = np.kron(np.eye(T), R)\n",
    "    return torch.tensor( -2 / rho * (-QQ @ E @ np.linalg.pinv(E.T @ QQ @ E + RR) @ E.T @ QQ @ F + F))\n",
    "\n",
    "theta_star = optimal_dual_map(A, B, Q, R, T, rho)\n",
    "\n",
    "def diff_theta(agent, actual_traj):\n",
    "    dual_diff_norm = torch.linalg.norm(\n",
    "        theta_star - agent.dual_network.weight.data.cpu(), 2\n",
    "    )\n",
    "    return dual_diff_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "16c80184-8f69-40cf-8125-e2c9dbcb94ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create agent\n",
    "from agent.layeredargs import LayeredArgs\n",
    "from agent.layered import LayeredAgent\n",
    "\n",
    "args = LayeredArgs(\n",
    "    seed=1,\n",
    "    torch_deterministic=True,\n",
    "    exp_name=f\"actor_critic\",\n",
    "    total_timesteps=100_000,\n",
    "    finetune_steps=10_000,\n",
    "    learning_starts=2_000, # have one batch before \n",
    "    buffer_size=10_000,\n",
    "    gamma=1., # No discount here\n",
    "    policy_noise= 0.01 / u_range,\n",
    "    noise_clip= 0.02 / u_range,\n",
    "    exploration_noise=0.05,\n",
    "    learning_rate=1e-3,\n",
    "    dual_optimizer='SGD',\n",
    "    dual_learning_rate=1e-2,\n",
    "    dual_batch_size=5,\n",
    "    cuda=True,\n",
    "    rho=rho,\n",
    "    tau=0.005,\n",
    ")\n",
    "\n",
    "agent = LayeredAgent(\n",
    "    nominal_env,\n",
    "    actor_creation_fn,\n",
    "    qnetwork_creation_fn,\n",
    "    dual_network_creation_fn,\n",
    "    ref_traj_gen_fn,\n",
    "    opt_cost_fn,\n",
    "    [('dual_diff', diff_theta)],\n",
    "    args\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17a177a1-7681-4567-951e-7880bd8f7676",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.learn()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b5e147-bc95-4fe4-83dc-c5947f50228c",
   "metadata": {},
   "source": [
    "## Linear Constrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d17ab49-fc07-4742-ba2a-9fc7345dd3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear env\n",
    "from env.dynamics import linear\n",
    "from env.costs import quadratic\n",
    "from controller.ilqr import ILQRHelper\n",
    "from env.gymenv import GymEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "92b6027a-ee64-40e0-8810-fb5c249915d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "seed = 3\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "_ = torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "25e6e7c6-14d2-4701-8e1c-5c3184589634",
   "metadata": {},
   "outputs": [],
   "source": [
    "Nx = 2\n",
    "Nu = 2\n",
    "T = 20\n",
    "RQratio = 1e-2\n",
    "\n",
    "# Define Dynamics\n",
    "dynamics = linear.sample_linear_dynamics(Nx, Nu, A_norm=.995)\n",
    "\n",
    "# Define Cost\n",
    "cost = quadratic.QuadraticCost(\n",
    "    torch.eye(dynamics.Nx),\n",
    "    RQratio * torch.eye(dynamics.Nu)\n",
    ")\n",
    "\n",
    "# Define how to sample x0\n",
    "reset_fn = lambda: torch.randn(dynamics.Nx)\n",
    "\n",
    "u_max = 10\n",
    "u_min = -10\n",
    "u_range = u_max - u_min\n",
    "nominal_env = GymEnv(dynamics, cost, reset_fn, T, ulim=[u_min, u_max])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0bb27031-eec0-4a34-8c65-98ec3c7100f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Specify the architecture of networks\n",
    "from agent.policy import LinearActor\n",
    "from agent.qnetwork import QuadraticQNetwork\n",
    "\n",
    "actor_creation_fn = lambda obs_dim, act_dim: LinearActor(obs_dim, act_dim)\n",
    "qnetwork_creation_fn = lambda obs_dim, act_dim: QuadraticQNetwork(obs_dim, act_dim)\n",
    "\n",
    "def dual_network_creation_fn(nominal_obs_dim, obs_dim):\n",
    "    dual_network = nn.Sequential(\n",
    "        nn.Linear(nominal_obs_dim, 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128, 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128, obs_dim - nominal_obs_dim),\n",
    "    )\n",
    "    dual_network[-1].weight.data.mul_(0.1)\n",
    "    dual_network[-1].bias.data.mul_(0.01)\n",
    "    return dual_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6852154b-8cf2-4873-8f6f-92a898159bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Specify how to sample new trajectories\n",
    "def ref_traj_gen_fn(x0s, T, agent, global_step):\n",
    "    b, Nx = x0s.shape\n",
    "    assert b == 1, \"Can only handle one initial condition at a time\"\n",
    "    x0 = x0s[0]\n",
    "    Q = agent.nominal_env.cost_fn.Q\n",
    "    K = agent.actor.gain.weight.data.cpu().numpy()\n",
    "    P = agent.qf1.get_P().cpu().numpy()\n",
    "    P = (P + P.T) / 2\n",
    "\n",
    "    r = cp.Variable(Nx * (T + 1))\n",
    "    v = agent.dual_network(torch.tensor(x0, device=agent.device)).detach().cpu().numpy()\n",
    "    if agent.args.no_dual:\n",
    "        v = np.zeros_like(v)\n",
    "    rtilde = r + v\n",
    "    xr = cp.hstack([x0, rtilde])\n",
    "    u = K @ xr\n",
    "    xru = cp.hstack([xr, u])\n",
    "    objective = cp.quad_form(r, np.kron(np.eye(T+1), Q.numpy()))\n",
    "    objective += cp.quad_form(xru, cp.psd_wrap(P))\n",
    "    prob = cp.Problem(\n",
    "        cp.Minimize(objective), [r[2:] >= -0.05]\n",
    "    )\n",
    "    try:\n",
    "        prob.solve()\n",
    "    except Exception as e:\n",
    "        print(f\"Error occured when solving the QP: {e}\")\n",
    "        print(np.linalg.svd(P))\n",
    "    return torch.tensor(rtilde.value).float().unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d64e8828-c20d-4ac3-a81b-022fec9657c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = nominal_env.dynamics.A.numpy()\n",
    "B = nominal_env.dynamics.B.numpy()\n",
    "Q = nominal_env.cost_fn.Q.numpy()\n",
    "R = nominal_env.cost_fn.R.numpy()\n",
    "nominal_Nx, Nu = B.shape\n",
    "T = nominal_env.T\n",
    "\n",
    "def opt_cost_fn(x0):\n",
    "    x_opt = cp.Variable((T+1, B.shape[0]))\n",
    "    u_opt = cp.Variable((T, B.shape[1]))\n",
    "    objective = 0\n",
    "    constr = [x_opt[0] == x0]\n",
    "    for t in range(T):\n",
    "        objective += cp.quad_form(x_opt[t], Q) + cp.quad_form(u_opt[t], R)\n",
    "        constr += [\n",
    "            x_opt[t+1] == A @ x_opt[t] + B @ u_opt[t]\n",
    "        ]\n",
    "    constr += [x_opt[1:] >= -0.05]\n",
    "    prob = cp.Problem(cp.Minimize(objective), constr)\n",
    "    prob.solve()\n",
    "    return objective.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c85740d4-e25c-4c35-8676-da241db9fa4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extra validation: optimal map theta\n",
    "def constr_violation(agent, actual_traj):\n",
    "    return (-0.05 - actual_traj[:, 2:]).clip(0, torch.inf).mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d5c51fab-9a11-47e9-9ef0-4db2e486b39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create agent\n",
    "from agent.layeredargs import LayeredArgs\n",
    "from agent.layered import LayeredAgent\n",
    "\n",
    "args = LayeredArgs(\n",
    "    seed=1,\n",
    "    torch_deterministic=True,\n",
    "    exp_name=f\"actor_critic\",\n",
    "    total_timesteps=150_000,\n",
    "    finetune_steps=250_000,\n",
    "    learning_starts=2_000, # have one batch before \n",
    "    buffer_size=10_000,\n",
    "    gamma=1., # No discount here\n",
    "    policy_noise=0.01 / u_range,\n",
    "    noise_clip=0.02 / u_range,\n",
    "    exploration_noise=0.01 / u_range,\n",
    "    learning_rate=3e-3,\n",
    "    dual_optimizer='Adam',\n",
    "    dual_learning_rate=3e-4,\n",
    "    dual_batch_size=40,\n",
    "    cuda=True,\n",
    "    rho=2,\n",
    "    tau=0.005,\n",
    "    no_dual=False\n",
    ")\n",
    "\n",
    "agent = LayeredAgent(\n",
    "    nominal_env,\n",
    "    actor_creation_fn,\n",
    "    qnetwork_creation_fn,\n",
    "    dual_network_creation_fn,\n",
    "    ref_traj_gen_fn,\n",
    "    opt_cost_fn,\n",
    "    [('val_constr_vio', constr_violation)],\n",
    "    args\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077a600d-cb60-4044-8dc9-cb2bf96cac03",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.learn()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642ad59b-f05a-49bb-974a-c348e1ef3ee5",
   "metadata": {},
   "source": [
    "## Unicycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1daaee58-fe09-431d-9770-c401f747c6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from env.dynamics import unicycle\n",
    "from env.costs import unicyclecost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9c4eefc0-7bc6-4707-a000-6a2d662b0599",
   "metadata": {},
   "outputs": [],
   "source": [
    "u_max = 10\n",
    "u_min = -10\n",
    "u_range = u_max - u_min\n",
    "RQratio = 1e-4\n",
    "T = 20\n",
    "dt = 0.1\n",
    "\n",
    "# Define Dynamics\n",
    "dynamics = unicycle.UnicycleDynamics(dt=dt)\n",
    "\n",
    "# Define Cost\n",
    "cost = quadratic.QuadraticCost(\n",
    "    torch.diag(torch.tensor([1.,1.,0.,0.])),\n",
    "    RQratio * torch.eye(dynamics.Nu)\n",
    ")\n",
    "tracking_mask = cost.Q.diag() != 0\n",
    "\n",
    "# Define how to sample x0\n",
    "def unicycle_reset_fn(sigma_x=1, sigma_theta=0.2):\n",
    "    x = torch.randn(4) * sigma_x\n",
    "    x[:2] = x[:2] / torch.linalg.norm(x[:2]) # Normalize the starting position to the unit circle\n",
    "    x[2] = 0\n",
    "    x[3] = torch.atan2(-x[1], -x[0]) + torch.randn(1) * sigma_theta\n",
    "    return x\n",
    "    \n",
    "#reset_fn = lambda: torch.randn(dynamics.Nx) / 5\n",
    "reset_fn = lambda: unicycle_reset_fn()\n",
    "\n",
    "nominal_env = GymEnv(dynamics, cost, reset_fn, T, ulim=[u_min, u_max])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f1c6e07f-316e-4f6b-82dd-b2dde793f66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Specify the architecture of networks\n",
    "from agent.policy import TransformerUnicycleActor\n",
    "from agent.qnetwork import UnicycleQNetwork\n",
    "\n",
    "actor_creation_fn = lambda obs_dim, act_dim: TransformerUnicycleActor(None, obs_dim, act_dim, u_max, u_min)\n",
    "qnetwork_creation_fn = lambda obs_dim, act_dim: UnicycleQNetwork(obs_dim, act_dim)\n",
    "\n",
    "def dual_network_creation_fn(nominal_obs_dim, obs_dim):\n",
    "    dual_var_dim = int((obs_dim - nominal_obs_dim))\n",
    "    dual_network = nn.Sequential(\n",
    "        nn.Linear(nominal_obs_dim, dual_var_dim),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(dual_var_dim, dual_var_dim),\n",
    "    )\n",
    "    dual_network[-1].weight.data.mul_(0.01)\n",
    "    dual_network[-1].bias.data.mul_(0.01)\n",
    "    return dual_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8fc7cc85-7f10-4ce6-8077-d03c0a8b6788",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Specify how to sample new trajectories\n",
    "def interpolate(x, n, noise_std):\n",
    "    steps = np.linspace(0, 1, n, endpoint=True).reshape(-1, 1)\n",
    "    interpolated_vectors = x * (1 - steps)\n",
    "    interpolated_vectors += np.random.randn(*interpolated_vectors.shape) * noise_std\n",
    "    return torch.tensor(interpolated_vectors.flatten()).float()\n",
    "\n",
    "#TODO: implementation below assumes that we only have ONE x0 per batch input\n",
    "def ref_traj_gen_fn(x0s, T, agent, global_step):\n",
    "    if global_step < 100_000:\n",
    "        ref = torch.vstack([interpolate(\n",
    "            xx[agent.validation_env.tracking_mask], T + 1, global_step / 100_000 * 0.03\n",
    "        ) for xx in x0s])\n",
    "        return ref\n",
    "    else:\n",
    "        x0 = torch.tensor(x0s[0], device=agent.device)\n",
    "        delta = torch.nn.Parameter(torch.zeros(2 * (T+1), device=agent.device))\n",
    "        ref_optimizer = torch.optim.SGD([delta], lr=3e-2)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            v = agent.dual_network(x0)\n",
    "            if agent.args.no_dual:\n",
    "                v.zero_()\n",
    "    \n",
    "        steps = torch.linspace(0, 1, T + 1).reshape(-1, 1).to(agent.device)\n",
    "        nominal = (x0[:2] * (1 - steps)).flatten()\n",
    "    \n",
    "        for it in range(agent.args.num_opt_steps):\n",
    "            ref_optimizer.zero_grad()\n",
    "            r = nominal + delta\n",
    "            rtilde = r + v\n",
    "            xr_tilde = torch.cat([x0, rtilde]).unsqueeze(0)\n",
    "            u = agent.actor(xr_tilde)\n",
    "            #TODO: This following line assumes that the state cost has Q=(1, 1, 0, 0)\n",
    "            cost = -agent.qf1(xr_tilde, u)[0][0] + (r * r).sum()\n",
    "            cost.backward()\n",
    "            ref_optimizer.step()\n",
    "        return rtilde.data.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "90536347-b235-418d-8fbb-6ce7aaf39f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimal cost function\n",
    "opt_cost_fn = lambda x0: 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "543c61b3-85a6-4d49-8468-9d318ac7325c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create agent\n",
    "from agent.layeredargs import LayeredArgs\n",
    "from agent.layered import LayeredAgent\n",
    "\n",
    "args = LayeredArgs(\n",
    "    seed=1,\n",
    "    torch_deterministic=True,\n",
    "    exp_name=f\"actor_critic\",\n",
    "    total_timesteps=500_000,\n",
    "    finetune_steps=10_000,\n",
    "    learning_starts=2_000, # have one batch before \n",
    "    buffer_size=60_000,\n",
    "    gamma=1., # No discount here\n",
    "    policy_noise=1e-3,\n",
    "    noise_clip=1e-2,\n",
    "    exploration_noise=4e-2,\n",
    "    learning_rate=1e-3,\n",
    "    dual_learning_rate=5e-3,\n",
    "    dual_batch_size=60,\n",
    "    cuda=True,\n",
    "    rho=5,\n",
    "    tau=0.005,\n",
    "    num_opt_steps=25,\n",
    "    dual_optimizer='SGD',\n",
    ")\n",
    "\n",
    "agent = LayeredAgent(\n",
    "    nominal_env,\n",
    "    actor_creation_fn,\n",
    "    qnetwork_creation_fn,\n",
    "    dual_network_creation_fn,\n",
    "    ref_traj_gen_fn,\n",
    "    opt_cost_fn,\n",
    "    [],\n",
    "    args,\n",
    "    tracking_mask,\n",
    "    warmstart_steps=100_000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989c777e-c61a-4f83-a660-422b83b587d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.learn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d8ef53-d7b4-4112-a8db-e6b0a59d6e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate nominal cost of the learned controller\n",
    "def sim_forward(td3agent, T):\n",
    "    x, _ = td3agent.envs.reset()\n",
    "    tc = 0\n",
    "    xtraj, utraj = [x], []\n",
    "    for t in range(T):\n",
    "        with torch.no_grad():\n",
    "            u = td3agent.actor(torch.tensor(x)).numpy()\n",
    "        x_, r, term, _, info = td3agent.envs.step(u)\n",
    "        x = x_\n",
    "        tc += r\n",
    "        xtraj.append(x)\n",
    "        utraj.append(u)\n",
    "    xtraj = np.vstack(xtraj)\n",
    "    utraj = np.vstack(utraj)\n",
    "    return (\n",
    "                np.vstack([xtraj[:-1], info['final_observation'][0]]),\n",
    "                utraj,\n",
    "                info['final_info'][0]['nominal_return'],\n",
    "                tc\n",
    "           )\n",
    "\n",
    "num_samples = 200\n",
    "ret = []\n",
    "agent.to('cpu')\n",
    "for _ in range(num_samples):\n",
    "    xtraj, utraj, r, tracking_cost = sim_forward(agent, T)\n",
    "    ret.append(r)\n",
    "np.mean(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a3670765-3a4c-41ec-9ba5-e545344429c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(8.898812, 3.638308)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the ILQR baseline\n",
    "from controller.ilqr import ILQRHelper\n",
    "ilqrsolver = ILQRHelper(nominal_env)\n",
    "ilqrsolver.T = nominal_env.T\n",
    "\n",
    "num_samples = 50\n",
    "per_sample_restart = 1\n",
    "costs = []\n",
    "for i in range(num_samples):\n",
    "    print(i, end=',')\n",
    "    x0, _ = nominal_env.reset()\n",
    "    c = []\n",
    "    for _ in range(per_sample_restart):\n",
    "        xtraj, utraj = ilqrsolver.solve(x0, max_iter=30)\n",
    "        c.append(\n",
    "            (xtraj[:-1, :2] * xtraj[:-1, :2]).sum() + (utraj * utraj).sum() * RQratio\n",
    "        )\n",
    "    costs.append(np.min(c))\n",
    "np.mean(costs), np.median(costs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7693fefa-a16d-4d84-aa6c-315133e7a937",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698a41c9-6383-4dbf-80d4-f26632590665",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797ea13c-3961-462b-81cc-8f641adf724f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ccb11e4-1da6-445c-9162-6d22b936a21f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b90577-ec75-482e-ba4c-c76e374e4a53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d11545-4879-4ca0-8e4e-d1c69512208c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e4e332-e838-4c19-ac6e-91c931b2dad1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e832a86d-5d8c-44fd-9cd8-73a988961b55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af57e31-08d3-49cd-bb50-2f998473f847",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314ca3f8-274f-41cf-a9f8-6be31f55da78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da682e7d-9eb3-4ed1-ab7a-237c1fae1d45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da1379e-0cb1-460b-9d28-be37375bcdce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94b0377-3a2a-4b5b-a22f-9ef9af1ae5a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd57667-e437-4d73-bc3d-8c05e2cfd802",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "layeredac",
   "language": "python",
   "name": "layeredac"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
